{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d096b134",
   "metadata": {},
   "source": [
    "# Import et chargement des csv"
   ]
  },
  {
   "cell_type": "code",
   "id": "12010352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:01:14.129007Z",
     "start_time": "2025-11-30T19:01:14.119963Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "a3703414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:01:14.143668Z",
     "start_time": "2025-11-30T19:01:14.135357Z"
    }
   },
   "source": "df_biaise = pd.read_csv(\"df_biais√©.csv\")",
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "ba7f3eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:01:14.150110Z",
     "start_time": "2025-11-30T19:01:14.146518Z"
    }
   },
   "source": "df_equi = pd.read_csv(\"df_non_biais√©.csv\")",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:01:14.154489Z",
     "start_time": "2025-11-30T19:01:14.152378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. S√©lection des Features (X) : La \"White List\"\n",
    "cols_to_keep = [\n",
    "    \"user_id\"\n",
    "    # --- A. IDENTIT√â & BIAIS (√Ä garder pour prouver la discrimination) ---\n",
    "    'age_group',                      # Source de l'√¢gisme\n",
    "    'sex',                      # Source du sexisme\n",
    "    'work_mode',                # Source du pr√©sent√©isme (Remote vs Office)\n",
    "    'mental_health_history',    # Source de la stigmatisation\n",
    "\n",
    "    # --- B. CONTEXTE PRO (L√©gitime pour la charge de travail) ---\n",
    "    'profession',\n",
    "    'work_hours',\n",
    "    'work_pressure',\n",
    "    'job_satisfaction',\n",
    "    'meetings_count',           # Indicateur de surcharge\n",
    "    'tasks_completed',          # Indicateur de productivit√©\n",
    "\n",
    "    # --- C. SANT√â MENTALE (L√©gitime pour la th√©rapie/vacances) ---\n",
    "    'stress_level',\n",
    "    'mood_score',\n",
    "    'anxiety_score',\n",
    "    'depression_score',\n",
    "    'perceived_stress_scale',\n",
    "    'sleep_quality',\n",
    "    'sleep_hours',\n",
    "\n",
    "    # --- D. PHYSIQUE & MODE DE VIE (L√©gitime pour Diet/Sport... ou biais√© ?) ---\n",
    "    'baseline_bmi',             # Indicateur m√©dical\n",
    "    'weight_kg',                # Souvent utilis√© pour le \"Fat shaming\" algorithmique\n",
    "    'diet_quality',\n",
    "    'exercise_habit',\n",
    "    'steps_count',\n",
    "    'caffeine_mg',             # Peut indiquer de la nervosit√©\n",
    "    \"cheat_meals_count\"\n",
    "]\n",
    "targets = ['intervention_vacation', 'intervention_diet_coaching', 'intervention_exercise_plan']\n"
   ],
   "id": "7a52b0382a5d69fd",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:01:14.179811Z",
     "start_time": "2025-11-30T19:01:14.157790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def split_user_equilibre(df, target_cols, user_col='user_id', test_size=0.2, n_essais=30):\n",
    "    \"\"\"\n",
    "    S√©pare le dataset en Train/Test en respectant DEUX crit√®res :\n",
    "    1. IMP√âRATIF : Aucun utilisateur (user_id) n'est coup√© en deux (Anti-Fuite).\n",
    "    2. OPTIMISATION : Cherche le split qui garde les m√™mes % d'interventions dans Train et Test.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Recherche du meilleur split parmis {n_essais} tentatives...\")\n",
    "\n",
    "    best_train = None\n",
    "    best_test = None\n",
    "    min_error = float('inf')\n",
    "    best_seed = 0\n",
    "\n",
    "    # On teste plusieurs graines al√©atoires (random_state)\n",
    "    for i in range(n_essais):\n",
    "        # On coupe par groupe d'utilisateurs\n",
    "        splitter = GroupShuffleSplit(test_size=test_size, n_splits=1, random_state=42 + i)\n",
    "\n",
    "        # On g√©n√®re les indices\n",
    "        try:\n",
    "            train_idx, test_idx = next(splitter.split(df, groups=df[user_col]))\n",
    "        except ValueError:\n",
    "            # S√©curit√© si user_id manque ou est mal format√©\n",
    "            print(\"‚ùå Erreur : Colonne user_id invalide ou manquante.\")\n",
    "            return None, None\n",
    "\n",
    "        temp_train = df.iloc[train_idx]\n",
    "        temp_test = df.iloc[test_idx]\n",
    "\n",
    "        # Calcul de l'erreur d'√©quilibre (Diff√©rence moyenne entre les taux Train et Test)\n",
    "        error_score = 0\n",
    "        for col in target_cols:\n",
    "            rate_train = temp_train[col].mean()\n",
    "            rate_test = temp_test[col].mean()\n",
    "            # On p√©nalise l'√©cart\n",
    "            error_score += abs(rate_train - rate_test)\n",
    "\n",
    "        # Si ce split est meilleur (plus √©quilibr√©) que les pr√©c√©dents, on le garde\n",
    "        if error_score < min_error:\n",
    "            min_error = error_score\n",
    "            best_train = temp_train.copy()\n",
    "            best_test = temp_test.copy()\n",
    "            best_seed = 42 + i\n",
    "\n",
    "        # Si l'erreur est tr√®s faible (moins de 0.5% cumul√©), on arr√™te, c'est parfait.\n",
    "        if min_error < 0.005 * len(target_cols):\n",
    "            print(f\"‚ú® Split parfait trouv√© pr√©matur√©ment (Essai {i+1})\")\n",
    "            break\n",
    "\n",
    "    print(f\"‚úÖ Meilleur split retenu (Seed {best_seed}). Ecart global : {min_error:.4f}\")\n",
    "\n",
    "    return best_train, best_test\n",
    "\n",
    "def verifier_qualite_split(train_df, test_df, target_cols):\n",
    "    \"\"\"Affiche un rapport de comparaison pour prouver l'√©quilibre.\"\"\"\n",
    "    print(\"\\nüìä AUDIT DU SPLIT (V√©rification √âquilibre)\")\n",
    "    print(f\"{'Cible':<30} | {'Train %':<10} | {'Test %':<10} | {'√âcart':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for col in target_cols:\n",
    "        tr = train_df[col].mean()\n",
    "        te = test_df[col].mean()\n",
    "        diff = abs(tr - te)\n",
    "\n",
    "        status = \"‚úÖ\" if diff < 0.015 else \"‚ö†Ô∏è\" # Alerte si √©cart > 1.5%\n",
    "        print(f\"{col:<30} | {tr:.1%}      | {te:.1%}      | {diff:.2%} {status}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# UTILISATION\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Configuration\n",
    "targets = ['intervention_vacation', 'intervention_diet_coaching', 'intervention_exercise_plan']\n",
    "\n",
    "# 2. Lancement du Split Intelligent\n",
    "# Assurez-vous que df_equi (votre dataset corrig√©) contient bien 'user_id'\n",
    "train_df, test_df = split_user_equilibre(df_equi, targets, user_col='user_id')\n",
    "\n",
    "# 3. V√©rification imm√©diate\n",
    "verifier_qualite_split(train_df, test_df, targets)\n",
    "\n",
    "# 4. V√©rification de l'√©tanch√©it√© (User ID)\n",
    "ids_train = set(train_df['user_id'].unique())\n",
    "ids_test = set(test_df['user_id'].unique())\n",
    "intersection = ids_train.intersection(ids_test)\n",
    "\n",
    "if len(intersection) == 0:\n",
    "    print(\"\\nüîí S√âCURIT√â : Aucune fuite d'utilisateur d√©tect√©e (Leakage = 0).\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ALERTE : {len(intersection)} utilisateurs sont dans les deux sets !\")"
   ],
   "id": "56cd7640cf72e90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Recherche du meilleur split parmis 30 tentatives...\n",
      "‚úÖ Meilleur split retenu (Seed 64). Ecart global : 0.0558\n",
      "\n",
      "üìä AUDIT DU SPLIT (V√©rification √âquilibre)\n",
      "Cible                          | Train %    | Test %     | √âcart     \n",
      "----------------------------------------------------------------------\n",
      "intervention_vacation          | 12.9%      | 13.3%      | 0.36% ‚úÖ\n",
      "intervention_diet_coaching     | 29.7%      | 28.9%      | 0.80% ‚úÖ\n",
      "intervention_exercise_plan     | 8.0%      | 3.6%      | 4.42% ‚ö†Ô∏è\n",
      "\n",
      "üîí S√âCURIT√â : Aucune fuite d'utilisateur d√©tect√©e (Leakage = 0).\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:01:14.198864Z",
     "start_time": "2025-11-30T19:01:14.183344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def construire_prompt_complet(row):\n",
    "    \"\"\"\n",
    "    Construit un prompt narratif complet int√©grant toutes les variables de cols_to_keep.\n",
    "    Structure par blocs logiques pour aider le LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. IDENTIT√â (Le sujet) ---\n",
    "    intro = (\n",
    "        f\"Employee Profile: {row['sex']}, {row['age_group']} group. \"\n",
    "        f\"History: Mental health history is {row['mental_health_history']}.\"\n",
    "    )\n",
    "\n",
    "    # --- 2. CONTEXTE PRO (La charge) ---\n",
    "    # On regroupe tout ce qui touche au travail\n",
    "    job = (\n",
    "        f\"Job Context: Works as {row['profession']} ({row['work_mode']}). \"\n",
    "        f\"Workload: {row['work_hours']} hours/day, {row['meetings_count']} meetings/day. \"\n",
    "        f\"Pressure: {row['work_pressure']}/10. Satisfaction: {row['job_satisfaction']}/10. \"\n",
    "        f\"Productivity: {row['tasks_completed']} tasks completed.\"\n",
    "    )\n",
    "\n",
    "    # --- 3. SANT√â MENTALE (L'√©tat interne) ---\n",
    "    # On regroupe les scores psy et le sommeil\n",
    "    mental = (\n",
    "        f\"Mental State: Stress level {row['stress_level']}/10 (Perceived: {row['perceived_stress_scale']}). \"\n",
    "        f\"Mood: {row['mood_score']}/10. Anxiety: {row['anxiety_score']}/10. Depression: {row['depression_score']}/10. \"\n",
    "        f\"Sleep: {row['sleep_hours']}h/night (Quality: {row['sleep_quality']}/10).\"\n",
    "    )\n",
    "\n",
    "    # --- 4. PHYSIQUE & MODE DE VIE (Le corps) ---\n",
    "    # On regroupe BMI, Poids, Habitudes\n",
    "    physique = (\n",
    "        f\"Physical Health: BMI {row['baseline_bmi']:.1f} (Weight: {row['weight_kg']}kg). \"\n",
    "        f\"Lifestyle: Diet is '{row['diet_quality']}' ({row['cheat_meals_count']} cheat meals). \"\n",
    "        f\"Activity: {row['exercise_habit']}, {row['steps_count']} steps/day. \"\n",
    "        f\"Caffeine: {row['caffeine_mg']}mg.\"\n",
    "    )\n",
    "\n",
    "    # --- 5. ASSEMBLAGE (Format Instruction) ---\n",
    "    prompt = (\n",
    "        f\"### Instruction:\\n\"\n",
    "        f\"Analyze the employee data below and predict necessary HR interventions \"\n",
    "        f\"(Vacation, Diet Coaching, Exercise Plan).\\n\\n\"\n",
    "\n",
    "        f\"### Input:\\n\"\n",
    "        f\"- {intro}\\n\"\n",
    "        f\"- {job}\\n\"\n",
    "        f\"- {mental}\\n\"\n",
    "        f\"- {physique}\\n\\n\"\n",
    "\n",
    "        f\"### Response:\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# --- APPLICATION ---\n",
    "print(\"üìù G√©n√©ration des prompts complets...\")\n",
    "\n",
    "# On l'applique sur Train et Test\n",
    "train_df['text'] = train_df.apply(construire_prompt_complet, axis=1)\n",
    "test_df['text'] = test_df.apply(construire_prompt_complet, axis=1)\n",
    "\n",
    "# V√©rification visuelle\n",
    "print(\"\\n--- Exemple de prompt g√©n√©r√© ---\")\n",
    "print(train_df['text'].iloc[0])"
   ],
   "id": "78b75d94f903e3f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù G√©n√©ration des prompts complets...\n",
      "\n",
      "--- Exemple de prompt g√©n√©r√© ---\n",
      "### Instruction:\n",
      "Analyze the employee data below and predict necessary HR interventions (Vacation, Diet Coaching, Exercise Plan).\n",
      "\n",
      "### Input:\n",
      "- Employee Profile: male, Adulte (35-50) group. History: Mental health history is none.\n",
      "- Job Context: Works as operations (onsite). Workload: 10.28 hours/day, 3 meetings/day. Pressure: low/10. Satisfaction: 6/10. Productivity: 3 tasks completed.\n",
      "- Mental State: Stress level 3/10 (Perceived: 14). Mood: 6/10. Anxiety: 4/10. Depression: 12/10. Sleep: 6.88h/night (Quality: 7/10).\n",
      "- Physical Health: BMI 23.7 (Weight: 58.37kg). Lifestyle: Diet is '5' (1 cheat meals). Activity: medium, 9262 steps/day. Caffeine: 327mg.\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chargement du mod√®le pr√©-entrain√©",
   "id": "a622b2eedb31bcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:01:18.180513Z",
     "start_time": "2025-11-30T19:01:14.207724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"distilgpt2\"\n",
    "targets = ['intervention_vacation', 'intervention_diet_coaching', 'intervention_exercise_plan']\n",
    "\n",
    "# 1. Configuration des labels (pour la lisibilit√©)\n",
    "id2label = {0: \"Vacation\", 1: \"Diet\", 2: \"Sport\"}\n",
    "label2id = {\"Vacation\": 0, \"Diet\": 1, \"Sport\": 2}\n",
    "\n",
    "# 2. Chargement du tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Configuration PAD token (Crucial pour GPT-2 qui n'en a pas par d√©faut)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Important : GPT-2 padding side est souvent √† droite, pour classification c'est ok.\n",
    "    # On s'assure que le mod√®le ignorera le padding.\n",
    "\n",
    "print(f\"\\nüîµ Chargement du mod√®le pour {len(targets)} cibles (Multi-Label)...\")\n",
    "\n",
    "# 3. Chargement du Mod√®le adapt√©\n",
    "model_baseline = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(targets),           # 3 labels de sortie\n",
    "    problem_type=\"multi_label_classification\", # <--- LE CHANGEMENT CL√â\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Configuration explicite du padding dans le mod√®le\n",
    "model_baseline.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© en mode MULTI-LABEL\")\n",
    "print(f\"   Architecture : {model_baseline.name_or_path}\")\n",
    "print(f\"   Param√®tres : {model_baseline.num_parameters():,}\")"
   ],
   "id": "e0326c801e21c60",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ Chargement du mod√®le pour 3 cibles (Multi-Label)...\n",
      "‚úÖ Mod√®le charg√© en mode MULTI-LABEL\n",
      "   Architecture : distilgpt2\n",
      "   Param√®tres : 81,914,880\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:01:18.210385Z",
     "start_time": "2025-11-30T19:01:18.202100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vos cibles\n",
    "targets = ['intervention_vacation', 'intervention_diet_coaching', 'intervention_exercise_plan']\n",
    "\n",
    "# Mapping pour la lisibilit√©\n",
    "id2label = {0: \"Vacation\", 1: \"Diet\", 2: \"Sport\"}\n",
    "label2id = {\"Vacation\": 0, \"Diet\": 1, \"Sport\": 2}\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. PR√âPARATION DES DONN√âES (Sp√©cifique Multi-Label)\n",
    "# ==============================================================================\n",
    "\n",
    "# A. Cr√©ation de la colonne 'labels' (Liste de [0, 1, 0])\n",
    "# Le Trainer a besoin d'une colonne unique nomm√©e \"labels\" contenant la liste des cibles\n",
    "train_df['labels'] = train_df[targets].values.tolist()\n",
    "test_df['labels'] = test_df[targets].values.tolist()\n",
    "\n",
    "# B. Conversion en Dataset Hugging Face\n",
    "# On ne garde que le texte (input) et les labels (target)\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'labels']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'labels']])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "    # Seuil de d√©cision (0.4 pour √™tre un peu plus sensible)\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='micro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}"
   ],
   "id": "acab68f1fa3e6c48",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FT",
   "id": "bd9cd2e5f38ad839"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:02:59.021571Z",
     "start_time": "2025-11-30T19:01:18.215031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "model_name = \"distilgpt2\"\n",
    "targets = ['intervention_vacation', 'intervention_diet_coaching', 'intervention_exercise_plan']\n",
    "id2label = {0: \"Vacation\", 1: \"Diet\", 2: \"Sport\"}\n",
    "label2id = {\"Vacation\": 0, \"Diet\": 1, \"Sport\": 2}\n",
    "\n",
    "# Nettoyage m√©moire Mac (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CALCUL DES POIDS (L'arme secr√®te contre le d√©s√©quilibre)\n",
    "# =============================================================================\n",
    "# On calcule combien de \"Non\" il y a pour chaque \"Oui\"\n",
    "num_positives = train_df[targets].sum().values\n",
    "num_negatives = len(train_df) - num_positives\n",
    "# Formule : Poids = N√©gatifs / Positifs\n",
    "# (Si j'ai 100 exemples et seulement 10 positifs, le poids sera 9. Le mod√®le sera puni 9x plus s'il rate un positif)\n",
    "pos_weights_calculated = torch.tensor(num_negatives / (num_positives + 1e-5), dtype=torch.float)\n",
    "\n",
    "print(f\"‚öñÔ∏è Poids calcul√©s pour √©quilibrer les classes : {pos_weights_calculated}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PR√âPARATION DES DONN√âES\n",
    "# =============================================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# GPT-2 n'a pas de pad_token par d√©faut, on utilise EOS\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "# Conversion & Tokenization\n",
    "train_ds = Dataset.from_pandas(train_df[['text', 'labels']])\n",
    "test_ds = Dataset.from_pandas(test_df[['text', 'labels']])\n",
    "\n",
    "train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# ‚ö†Ô∏è CRUCIAL : Conversion des labels en Float pour √©viter le crash RuntimeError\n",
    "def format_labels(batch):\n",
    "    batch['labels'] = [list(map(float, l)) for l in batch['labels']]\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(format_labels, batched=True)\n",
    "test_ds = test_ds.map(format_labels, batched=True)\n",
    "\n",
    "# Format PyTorch\n",
    "cols = ['input_ids', 'attention_mask', 'labels']\n",
    "train_ds.set_format(type='torch', columns=cols)\n",
    "test_ds.set_format(type='torch', columns=cols)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MOD√àLE\n",
    "# =============================================================================\n",
    "model_FT = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(targets),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model_FT.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TRAINER PERSONNALIS√â (Weighted Loss)\n",
    "# =============================================================================\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model_FT, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # 1. R√©cup√©rer et retirer les labels des inputs (√©vite que le mod√®le calcule sa loss interne)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        # 2. Forward pass (Le mod√®le retourne les logits)\n",
    "        outputs = model_FT(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # 3. CORRECTION DU TYPE : On force les labels en Float32 (C'est √ßa qui manquait/plantait)\n",
    "        labels = labels.to(torch.float32)\n",
    "\n",
    "        # 4. Envoi des poids calcul√©s sur le bon device (GPU/MPS)\n",
    "        weights = pos_weights_calculated.to(logits.device)\n",
    "\n",
    "        # 5. Calcul de la Loss\n",
    "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- RELANCE DE L'ENTRA√éNEMENT ---\n",
    "# On s'assure de vider le cache avant\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Configuration l√©g√®re pour Mac (Batch=4 + Gradient Accumulation)\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,    # Petit batch pour √©viter le OOM\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,    # Simule un batch de 16\n",
    "    gradient_checkpointing=True,      # Sauve la m√©moire\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    fp16=False # MPS n'aime pas toujours le FP16, on reste en FP32 par s√©curit√©\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model_FT,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ D√©marrage du Fine-Tuning (Version Corrig√©e)...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- OPTIMISATION DU SEUIL ---\n",
    "print(\"\\nüîé Recherche du meilleur seuil...\")\n",
    "preds_output = trainer.predict(test_ds)\n",
    "logits = preds_output.predictions\n",
    "# On s'assure que les labels de test sont bien format√©s\n",
    "true_labels = np.array(test_ds['labels'])\n",
    "\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "\n",
    "for t in np.arange(0.1, 0.9, 0.05):\n",
    "    preds_t = (probs > t).astype(int)\n",
    "    # On utilise 'micro' pour avoir une vue globale\n",
    "    score = f1_score(true_labels, preds_t, average='micro')\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"‚úÖ Meilleur Seuil : {best_thresh:.2f}\")\n",
    "print(f\"üåü F1-Score Final : {best_f1:.2%}\")"
   ],
   "id": "925ad849b80a1846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Poids calcul√©s pour √©quilibrer les classes : tensor([ 6.7536,  2.3648, 11.4419])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 535/535 [00:00<00:00, 13370.71 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:00<00:00, 8808.22 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 535/535 [00:00<00:00, 202870.68 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:00<00:00, 53998.33 examples/s]\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ D√©marrage du Fine-Tuning (Version Corrig√©e)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 01:37, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.201200</td>\n",
       "      <td>1.428737</td>\n",
       "      <td>0.108434</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.214100</td>\n",
       "      <td>1.167949</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.152600</td>\n",
       "      <td>1.194099</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.951800</td>\n",
       "      <td>1.175739</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.264901</td>\n",
       "      <td>0.176991</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "ca958aab371d27683c2083bd19bdeabb"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Recherche du meilleur seuil...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "8e79f717a37dc94e5232cf405e2de77d"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Meilleur Seuil : 0.30\n",
      "üåü F1-Score Final : 28.14%\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "05b7cd25",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6ed05a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:03:36.710176Z",
     "start_time": "2025-11-30T19:02:59.065627Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "model_name = \"distilgpt2\"\n",
    "targets = ['intervention_vacation', 'intervention_diet_coaching', 'intervention_exercise_plan']\n",
    "id2label = {0: \"Vacation\", 1: \"Diet\", 2: \"Sport\"}\n",
    "label2id = {\"Vacation\": 0, \"Diet\": 1, \"Sport\": 2}\n",
    "\n",
    "# Nettoyage m√©moire (Mac/GPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# --- 2. PR√âPARATION DES DONN√âES (Vital pour √©viter l'erreur de Type) ---\n",
    "print(\"üîÑ Pr√©paration et Tokenization...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# GPT-2 n'a pas de pad token par d√©faut\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "# On recr√©e les datasets HuggingFace √† partir des DataFrames\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'labels']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'labels']])\n",
    "\n",
    "# Tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ‚ö†Ô∏è CORRECTION TYPE : Conversion explicite des labels en Float32\n",
    "def force_float_labels(batch):\n",
    "    batch['labels'] = [list(map(float, label)) for label in batch['labels']]\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(force_float_labels, batched=True)\n",
    "test_dataset = test_dataset.map(force_float_labels, batched=True)\n",
    "\n",
    "# Formatage PyTorch (Essentiel)\n",
    "cols_model = ['input_ids', 'attention_mask', 'labels']\n",
    "train_dataset.set_format(type='torch', columns=cols_model)\n",
    "test_dataset.set_format(type='torch', columns=cols_model)\n",
    "\n",
    "# --- 3. CALCUL DES POIDS (Si ce n'est pas d√©j√† fait plus haut) ---\n",
    "num_positives = train_df[targets].sum().values\n",
    "num_negatives = len(train_df) - num_positives\n",
    "pos_weights_tensor = torch.tensor(num_negatives / (num_positives + 1e-5), dtype=torch.float)\n",
    "print(f\"‚öñÔ∏è Poids utilis√©s : {pos_weights_tensor}\")\n",
    "\n",
    "# --- 4. MOD√àLE & LoRA ---\n",
    "print(f\"\\nüîµ Chargement de {model_name} pour LoRA...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(targets),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Configuration LoRA sp√©cifique pour GPT-2\n",
    "print(\"‚ú® Application de la configuration LoRA (Cibles GPT-2)...\")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # ‚ö†Ô∏è CIBLE SP√âCIFIQUE GPT-2 (Attention projection layer)\n",
    "    target_modules=['c_attn']\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model, peft_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "# --- 5. TRAINER PERSONNALIS√â (Weighted + Type Safety) ---\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model_lora, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Extraction des labels\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # ‚ö†Ô∏è S√âCURIT√â : On s'assure que tout est sur le bon device et au bon format\n",
    "        labels = labels.to(logits.device, dtype=torch.float32)\n",
    "        weights = pos_weights_tensor.to(logits.device)\n",
    "\n",
    "        # Calcul de la Loss Pond√©r√©e\n",
    "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- 6. M√âTRIQUES ---\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Sigmoid pour multi-label\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='micro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# --- 7. LANCEMENT ---\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-4,              # LR plus √©lev√© pour LoRA\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,  # Batch plus grand car LoRA est l√©ger\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ D√©marrage de l'entra√Ænement LoRA...\")\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Pr√©paration et Tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 535/535 [00:00<00:00, 13168.58 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:00<00:00, 8540.28 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 535/535 [00:00<00:00, 197722.50 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:00<00:00, 56504.99 examples/s]\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Poids utilis√©s : tensor([ 6.7536,  2.3648, 11.4419])\n",
      "\n",
      "üîµ Chargement de distilgpt2 pour LoRA...\n",
      "‚ú® Application de la configuration LoRA (Cibles GPT-2)...\n",
      "trainable params: 297,216 || all params: 82,212,096 || trainable%: 0.3615\n",
      "\n",
      "üöÄ D√©marrage de l'entra√Ænement LoRA...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.178800</td>\n",
       "      <td>1.049548</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.292135</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.125700</td>\n",
       "      <td>1.089191</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.137097</td>\n",
       "      <td>0.447368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.089300</td>\n",
       "      <td>1.095736</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.203593</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.447368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "d4a984213d9b2a2707747f945d7558e5"
     }
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102, training_loss=1.1258072058359783, metrics={'train_runtime': 36.2461, 'train_samples_per_second': 44.281, 'train_steps_per_second': 2.814, 'total_flos': 52791860920320.0, 'train_loss': 1.1258072058359783, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "fdac0204",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:03:36.741552Z",
     "start_time": "2025-11-30T19:03:36.738398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- √âTAPE 1 : CALCUL DYNAMIQUE ET AJUST√â ---\n",
    "\n",
    "# Facteur de sur-p√©nalisation (multiplication par 2.0 pour forcer la pr√©diction)\n",
    "SUR_PENALISATION_FACTOR = 2.0\n",
    "\n",
    "# Calcul du poids exact (N√©gatifs / Positifs)\n",
    "num_positives = train_df[targets].sum().values\n",
    "num_negatives = len(train_df) - num_positives\n",
    "pos_weights_calculated_raw = num_negatives / (num_positives + 1e-5)\n",
    "\n",
    "# Tenseur de poids final utilis√© par le Trainer\n",
    "pos_weights_calculated = torch.tensor(\n",
    "    pos_weights_calculated_raw * SUR_PENALISATION_FACTOR,\n",
    "    dtype=torch.float\n",
    ")\n",
    "print(f\"Poids calcul√©s pour la Distillation (x{SUR_PENALISATION_FACTOR}) : {pos_weights_calculated}\")"
   ],
   "id": "61dad54cea1d98cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids calcul√©s pour la Distillation (x2.0) : tensor([13.5072,  4.7296, 22.8837])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:04:24.430094Z",
     "start_time": "2025-11-30T19:03:36.750745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- CORRECTION : TOKENIZATION PR√âALABLE ---\n",
    "# Le dataset doit √™tre transform√© en nombres avant d'√™tre pass√© au mod√®le\n",
    "\n",
    "# 1. On s'assure que le tokenizer est bien configur√©\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "print(\"üîÑ Tokenization des donn√©es pour la distillation...\")\n",
    "\n",
    "# 2. On applique la tokenization si ce n'est pas d√©j√† fait\n",
    "if \"input_ids\" not in train_dataset.column_names:\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 3. IMPORTANT : On met les colonnes au format PyTorch\n",
    "cols_model = ['input_ids', 'attention_mask', 'labels']\n",
    "train_dataset.set_format(type='torch', columns=cols_model)\n",
    "test_dataset.set_format(type='torch', columns=cols_model)\n",
    "\n",
    "print(\"‚úÖ Donn√©es pr√™tes (input_ids g√©n√©r√©s).\")\n",
    "# --- √âTAPE 1 : PR√â-CALCUL DES LOGITS DU PROFESSEUR (OFFLINE) ---\n",
    "print(\"üîÆ G√©n√©ration des logits du Professeur (Offline)...\")\n",
    "\n",
    "# On passe le teacher en mode eval et sur le device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "teacher_model = model_baseline.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "def add_teacher_logits(batch):\n",
    "    # On pr√©pare les inputs pour le teacher\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
    "        \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(**inputs)\n",
    "\n",
    "    # On retourne les logits (scores bruts) convertis en listes python\n",
    "    return {\"teacher_logits\": outputs.logits.cpu().numpy()}\n",
    "\n",
    "# On applique √ßa sur le dataset (map)\n",
    "# batch_size petit pour √©viter OOM\n",
    "train_dataset_distill = train_dataset.map(add_teacher_logits, batched=True, batch_size=8)\n",
    "test_dataset_distill = test_dataset.map(add_teacher_logits, batched=True, batch_size=8)\n",
    "\n",
    "# On lib√®re la m√©moire du teacher (CRUCIAL sur Mac)\n",
    "del teacher_model\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Logits g√©n√©r√©s. Le Professeur a quitt√© la salle.\")\n",
    "\n",
    "# --- √âTAPE 2 : FORMATAGE DES DONN√âES ---\n",
    "# On doit s'assurer que 'teacher_logits' est bien un tenseur PyTorch\n",
    "cols = ['input_ids', 'attention_mask', 'labels', 'teacher_logits']\n",
    "train_dataset_distill.set_format(type='torch', columns=cols)\n",
    "test_dataset_distill.set_format(type='torch', columns=cols)\n",
    "\n",
    "\n",
    "# --- √âTAPE 3 : TRAINER POUR DISTILLATION OFFLINE ---\n",
    "class OfflineDistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, pos_weights=None, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weights = pos_weights\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # 1. R√©cup√©ration des donn√©es\n",
    "        labels = inputs.pop(\"labels\").float() # Cibles r√©elles (Ground Truth)\n",
    "        teacher_logits = inputs.pop(\"teacher_logits\") # Cibles du professeur (Soft Targets)\n",
    "\n",
    "        # 2. Forward de l'√©l√®ve\n",
    "        outputs = model(**inputs)\n",
    "        student_logits = outputs.logits\n",
    "\n",
    "        # 3. Loss √âtudiante (BCE avec poids pour l'imbalance)\n",
    "        if self.pos_weights is not None:\n",
    "             weights = self.pos_weights.to(student_logits.device)\n",
    "             loss_ce_fct = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "        else:\n",
    "             loss_ce_fct = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        loss_ce = loss_ce_fct(student_logits, labels)\n",
    "\n",
    "        # 4. Loss de Distillation (KL Divergence)\n",
    "        # Elle compare la distribution de l'√©l√®ve √† celle du prof\n",
    "        loss_kd = F.kl_div(\n",
    "            F.log_softmax(student_logits / self.temperature, dim=-1),\n",
    "            F.softmax(teacher_logits / self.temperature, dim=-1),\n",
    "            reduction=\"batchmean\"\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        # 5. Loss Totale\n",
    "        # Souvent on met plus de poids sur la distillation (alpha faible pour CE)\n",
    "        loss = (self.alpha * loss_ce) + ((1 - self.alpha) * loss_kd)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "# --- √âTAPE 4 : LANCEMENT AVEC SUIVI DES M√âTRIQUES ---\n",
    "\n",
    "# Recr√©ation d'un mod√®le √©l√®ve vierge\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(targets),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    n_layer=2,\n",
    "    n_head=4,\n",
    "    n_embd=256\n",
    ")\n",
    "student_config.pad_token_id = tokenizer.pad_token_id\n",
    "student_model = AutoModelForSequenceClassification.from_config(student_config)\n",
    "\n",
    "# Configuration pour avoir le tableau\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=10,\n",
    "\n",
    "    # Gestion m√©moire\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    # ‚ö†Ô∏è Param√®tres pour l'affichage du tableau\n",
    "    eval_strategy=\"epoch\",     # √âvaluer √† la fin de chaque √©poque\n",
    "    save_strategy=\"epoch\",     # Sauvegarder √† la fin de chaque √©poque\n",
    "    logging_strategy=\"epoch\",  # Afficher les logs (Training Loss) √† chaque √©poque\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\", # La m√©trique √† surveiller\n",
    "\n",
    "    remove_unused_columns=False, # Indispensable pour la distillation offline\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# On d√©finit une fonction de m√©trique adapt√©e qui nettoie les inputs si n√©cessaire\n",
    "# (Le Trainer passe parfois des tuples √† compute_metrics en distillation)\n",
    "def compute_metrics_distill(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # predictions est souvent un tuple (logits_eleve, hidden_states...)\n",
    "    # on ne garde que le premier √©l√©ment (logits)\n",
    "    if isinstance(predictions, tuple):\n",
    "        logits = predictions[0]\n",
    "    else:\n",
    "        logits = predictions\n",
    "\n",
    "    # Transformation sigmoid pour multi-label\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    y_pred = (probs > 0.5).astype(int)\n",
    "\n",
    "    # Calculs classiques\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, y_pred, average='micro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, y_pred)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = OfflineDistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_distill,\n",
    "    eval_dataset=test_dataset_distill, # C'est ici que l'√©valuation se fera\n",
    "    compute_metrics=compute_metrics_distill, # On utilise la fonction robuste\n",
    "    pos_weights=pos_weights_calculated,\n",
    "    alpha=0.5,\n",
    "    temperature=4.0\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ D√©marrage de la Distillation Offline avec tableau de suivi...\")\n",
    "trainer.train()"
   ],
   "id": "b0f31a940ec81c43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tokenization des donn√©es pour la distillation...\n",
      "‚úÖ Donn√©es pr√™tes (input_ids g√©n√©r√©s).\n",
      "üîÆ G√©n√©ration des logits du Professeur (Offline)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 535/535 [00:04<00:00, 123.16 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83/83 [00:00<00:00, 114.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logits g√©n√©r√©s. Le Professeur a quitt√© la salle.\n",
      "\n",
      "üöÄ D√©marrage de la Distillation Offline avec tableau de suivi...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 00:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.879700</td>\n",
       "      <td>0.856816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264808</td>\n",
       "      <td>0.152610</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.827100</td>\n",
       "      <td>0.856115</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.798100</td>\n",
       "      <td>0.836084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241509</td>\n",
       "      <td>0.140969</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.749200</td>\n",
       "      <td>0.875573</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.688200</td>\n",
       "      <td>0.993348</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>0.851199</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.447368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>1.003501</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.143885</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.598400</td>\n",
       "      <td>1.163609</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>1.245316</td>\n",
       "      <td>0.265060</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.533300</td>\n",
       "      <td>1.283494</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "fdcd58cd21b17a2a47788e2d27f1b217"
     }
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=340, training_loss=0.6917049744549919, metrics={'train_runtime': 20.6193, 'train_samples_per_second': 259.465, 'train_steps_per_second': 16.489, 'total_flos': 6495191040000.0, 'train_loss': 0.6917049744549919, 'epoch': 10.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T19:04:24.484456Z",
     "start_time": "2025-11-30T19:04:24.483198Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "33e104d87a8135e3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethics_proj_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
